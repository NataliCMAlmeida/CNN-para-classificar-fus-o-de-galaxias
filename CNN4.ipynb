{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222b13e-c62a-4e18-be9e-60aa639368cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As imagens normalmente tem 1 ou 3 canais, mas essa simulação só usa 2\n",
    "#Então, para não aparecer um monte de avisos, coloquei essa parte\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a879712-81ba-4c86-9411-7bbcfbdddcc2",
   "metadata": {},
   "source": [
    "Obtenção dos dados da variável X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa77bb-0051-4959-8126-e8e4c8ab23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import simple_norm  # Import simple_norm\n",
    "\n",
    "# Caminho para o arquivo FITS\n",
    "file_path = r\"C:\\Users\\stefa\\OneDrive\\Documentos\\NATALI TCC\\hlsp_deepmerge_hst_acs-wfc3_illustris-z2_f814w-f160w_v1_sim-pristine.fits\"\n",
    "\n",
    "# Abre o arquivo FITS\n",
    "hdulist = fits.open(file_path)\n",
    "\n",
    "# Extrai as imagens do HDU[0] e imprime o shape\n",
    "images = hdulist[0].data\n",
    "print(f\"Shape das imagens: {images.shape}\")  \n",
    "# valor esperado: (15426, 2, 75, 75)\n",
    "\n",
    "# Selecionar 3 imagens aleatórias de cada filtro [ F160W (índice=1) e F814W (índice=0)]\n",
    "\n",
    "example_ids = np.random.choice(hdulist[1].data.shape[0], 2)\n",
    "print(example_ids)\n",
    "\n",
    "examples_f160w = [hdulist[0].data[j, 1, :, :] for j in example_ids]\n",
    "examples_f814w = [hdulist[0].data[j, 0, :, :] for j in example_ids]\n",
    "\n",
    "# Inicializar a figura\n",
    "fig = plt.figure(figsize=(16, 16))  # Ajuste a altura para separar as imagens\n",
    "\n",
    "# Loop pelas imagens selecionadas aleatoriamente e plotar com rótulos\n",
    "for i, (image_f160w, image_f814w) in enumerate(zip(examples_f160w, examples_f814w)):\n",
    "    # F160W\n",
    "    ax1 = fig.add_subplot(8, 4, i * 2 + 1)  # Ajuste o layout para 8 linhas e 4 colunas\n",
    "    norm1 = simple_norm(image_f160w, 'log', max_percent=99.75)\n",
    "    ax1.imshow(image_f160w, aspect='equal', cmap='binary_r', norm=norm1)\n",
    "    ax1.set_title('F160W') \n",
    "    ax1.axis('off')\n",
    "\n",
    "    # F814W\n",
    "    ax2 = fig.add_subplot(8, 4, i * 2 + 2)  # Ajuste o layout para 8 linhas e 4 colunas\n",
    "    norm2 = simple_norm(image_f814w, 'log', max_percent=99.75)\n",
    "    ax2.imshow(image_f814w, aspect='equal', cmap='binary_r', norm=norm2)\n",
    "    ax2.set_title('F814W')  \n",
    "    ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()  # Ajuste o layout para evitar sobreposição dos títulos\n",
    "plt.show()\n",
    "\n",
    "# Salve todas as imagens, de ambos os filtros, na variável X\n",
    "X = images\n",
    "print(X.shape)\n",
    "\n",
    "hdulist.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6685ae2-22b0-4f83-a5e5-7c80e1c9e961",
   "metadata": {},
   "source": [
    "Processo de aumento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178d574-3113-45bd-815c-70d57aac7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Transpõe para o formato esperado pelo ImageDataGenerator\n",
    "X = X.transpose(0, 2, 3, 1) \n",
    "print(\"Após transposição:\", X.shape)\n",
    "\n",
    "# Converte os dados para o tipo float32\n",
    "X = X.astype(np.float32)\n",
    "print(\"Após conversão para float32:\", X.shape)\n",
    "\n",
    "# Normaliza os dados originais\n",
    "X = X / 255.0  # Normalizando para o intervalo [0, 1]\n",
    "print(\"Após normalização:\", X.shape)\n",
    "\n",
    "#Criação do ImageDataGenerator (sem normalização, pois já está normalizado)\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  # Rotação de até 20 graus\n",
    "    width_shift_range=0.1,  # Deslocamento horizontal de até 10% da largura\n",
    "    height_shift_range=0.1,  # Deslocamento vertical de até 10% da altura\n",
    "    shear_range=0.1,  # Distorção por cisalhamento de até 10%\n",
    "    zoom_range=0.1,  # Zoom de até 10%\n",
    "    horizontal_flip=True,  # Reflexão horizontal com 50% de probabilidade\n",
    "    fill_mode='nearest'  # Preenchimento de pixels faltantes\n",
    ")\n",
    "\n",
    "# Geração de imagens aumentadas em batches e salvamento em arquivo .fits\n",
    "num_augmented_images = int(15426 * 1.5)  # Aumenta em 50% (1.5 = 1 + 0.5)\n",
    "X_augmented = []\n",
    "for batch_index, batch in enumerate(datagen.flow(X, batch_size=32, shuffle=True)):\n",
    "    # Reorganiza as dimensões\n",
    "    batch = batch.reshape(-1, 2, 75, 75)\n",
    "    X_augmented.extend(batch)\n",
    "    # Verifica se o número de imagens aumentadas atingiu o limite\n",
    "    if len(X_augmented) >= num_augmented_images:\n",
    "        break\n",
    "\n",
    "# Converte as listas para arrays NumPy\n",
    "X_augmented = np.array(X_augmented)[:num_augmented_images]\n",
    "print(\"Após aumento de dados:\", X_augmented.shape)\n",
    "\n",
    "# Salva como um arquivo\n",
    "np.save(\"X_augmented.npy\", X_augmented)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f73bb00-af76-450f-8bf2-7d7163c91cff",
   "metadata": {},
   "source": [
    "Obtenção dos dados da variável Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9727b078-e451-4ca1-a907-541abc9c7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as fits\n",
    "import numpy as np\n",
    "\n",
    "# Caminho para o arquivo FITS\n",
    "file_path = r\"C:\\Users\\stefa\\OneDrive\\Documentos\\NATALI TCC\\hlsp_deepmerge_hst_acs-wfc3_illustris-z2_f814w-f160w_v1_sim-pristine.fits\"\n",
    "\n",
    "# Abre o arquivo FITS\n",
    "hdulist = fits.open(file_path)\n",
    "\n",
    "#Extrai os labels do HDU[1] e imprime o shape\n",
    "labels = hdulist[1].data\n",
    "print(f\"Shape dos labels: {labels.shape}\")  \n",
    "# valor esperado: (15426,)\n",
    "\n",
    "# Salva os labels na variável Y\n",
    "Y = labels\n",
    "\n",
    "# Converte as listas para arrays NumPy e printe o shape\n",
    "Y = np.array(Y)\n",
    "print(Y.shape)\n",
    "\n",
    "# Salva o array Y em um arquivo .npy\n",
    "np.save(\"Y.npy\", Y)\n",
    "\n",
    "# Fecha o arquivo FITS\n",
    "hdulist.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f431d0d6-ecaf-42d5-aeb4-7bb1cb6d02a4",
   "metadata": {},
   "source": [
    "Extração das listas de Merge e Nonmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670d9a1-e88b-48b1-a71e-1ee76c5ffed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "\n",
    "# Caminho do arquivo FITS\n",
    "file_fits = \"C:\\\\Users\\\\stefa\\\\OneDrive\\\\Documentos\\\\NATALI TCC\\\\hlsp_deepmerge_hst_acs-wfc3_illustris-z2_f814w-f160w_v1_sim-pristine.fits\"\n",
    "\n",
    "# Abrir o arquivo FITS\n",
    "hdulist = fits.open(file_fits)\n",
    "\n",
    "# Extrair os labels\n",
    "data_hdu1 = hdulist[1].data\n",
    "\n",
    "# Criar as listas de merge e non-merge\n",
    "list_of_mergers = np.where(data_hdu1 == 1.)[0]\n",
    "list_of_nonmergers = np.where(data_hdu1 == 0.)[0]\n",
    "\n",
    "# Salvar os arrays em arquivos .npy\n",
    "np.save(\"mergers.npy\", list_of_mergers)\n",
    "np.save(\"nonmergers.npy\", list_of_nonmergers)\n",
    "\n",
    "# Fechar o arquivo FITS\n",
    "hdulist.close()\n",
    "\n",
    "print(\"Arrays salvos em mergers.npy e nonmergers.npy.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "136bf2fc-0206-4e1a-850e-9ffb2c526585",
   "metadata": {},
   "source": [
    "Concatenação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928655f8-8a6f-4640-96ce-dafd7af9f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Carrega os arrays X_augmented e Y\n",
    "X_augmented = np.load(\"X_augmented.npy\")\n",
    "Y = np.load(\"Y.npy\")\n",
    "#valor esperado: X_augmented = (23139, 75, 75, 2) e Y=(15426,)\n",
    "\n",
    "# Verifica os shapes\n",
    "print(f\"Shape de X_augmented: {X_augmented.shape}\") \n",
    "print(f\"Shape de Y: {Y.shape}\")\n",
    "\n",
    "# Seleciona aleatoriamente 15426 imagens de X_augmented\n",
    "num_imagens = len(Y)  # Usar o tamanho de Y para garantir correspondência\n",
    "indices_aleatorios = random.sample(range(len(X_augmented)), num_imagens)\n",
    "X_augmented_aleatorio = X_augmented[indices_aleatorios]\n",
    "\n",
    "# Cria um novo array com as imagens e labels\n",
    "data = np.zeros((num_imagens, *X_augmented_aleatorio.shape[1:]), dtype=X_augmented.dtype)\n",
    "data[:, :] = X_augmented_aleatorio\n",
    "\n",
    "# Converta os labels para o tipo de dados correto\n",
    "Y_type = X_augmented.dtype  # Obter o tipo de dados das imagens\n",
    "Y = Y.astype(Y_type)  # Converta os labels para o tipo de dados das imagens\n",
    "\n",
    "data[:, -1] = Y  # Adiciona os labels na última coluna\n",
    "\n",
    "# Salva o array em data.npy\n",
    "np.save(\"data.npy\", data)\n",
    "\n",
    "print(\"Arquivo salvo em data.npy\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bfa2b36-f010-4b87-9217-1a4763035fce",
   "metadata": {},
   "source": [
    "Divisão em Teste, Treino e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aed159-11b8-4949-8d2e-0b93e29541d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carrega o array concatenado\n",
    "data = np.load(\"data.npy\")\n",
    "\n",
    "# Extrai as imagens e os labels\n",
    "X = data  # X_augmented já possui a forma correta das imagens\n",
    "Y = data[:, -1]  # Assumindo que o label é a última coluna\n",
    "\n",
    "# Divisão dos dados em treino, teste e validação\n",
    "random_seed = 42\n",
    "X_train, X_rem, Y_train, Y_rem = train_test_split(X, Y, test_size=0.3, random_state=random_seed)\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_rem, Y_rem, test_size=1/3, random_state=random_seed)\n",
    "\n",
    "# Imprime o shape dos testes, treinos e validações de X e de Y\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"Y_val shape:\", Y_val.shape)\n",
    "\n",
    "# Salva os dados como arquivos .npy\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"Y_train.npy\", Y_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"Y_test.npy\", Y_test)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"Y_val.npy\", Y_val)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0869daa-1ac6-4e24-a9a6-efc9e780ddf6",
   "metadata": {},
   "source": [
    "Arquitetura da Rede Neural Convolucionária (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8400e0-1b0a-41d0-ba67-08f96544ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80568a1c-c7ee-42a7-960e-aada5bf503a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a camada de entrada (com 2 canais)\n",
    "input_tensor = layers.Input(shape=(75, 75, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e968c-f974-43ae-be99-9a2326c238cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camadas convolucionais\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001))(input_tensor)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# Camadas densas\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.0001, l2=0.0001))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Camada de saída\n",
    "output_tensor = layers.Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133cc6b-e8d6-4eec-bb5e-6f1d56a20246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792a1af-6ef4-4cf8-a0dd-2fd418551ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros de compilação\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "metrics = ['accuracy']\n",
    "loss = 'binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5f757-5874-4f59-ac58-b99008c07fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1166f4-6de4-4104-8a14-bd4a8c517ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para salvar checkpoints\n",
    "checkpoint_filepath = 'melhor_modelo_pesos.weights.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d780195c-dfce-4774-bcde-6d2626f5315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    patience=30, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=30,\n",
    "    min_lr=1e-05\n",
    ")\n",
    "\n",
    "# Combine callbacks into a list:\n",
    "callbacks = [checkpoint, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc26d0-d14f-40ad-b9b4-85a43b8e2c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados dos arquivos .npy\n",
    "X_train = np.load(\"X_train.npy\")\n",
    "Y_train = np.load(\"Y_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "Y_test = np.load(\"Y_test.npy\")\n",
    "X_val = np.load(\"X_val.npy\")\n",
    "Y_val = np.load(\"Y_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e00be-e3aa-4372-b5b2-b759923d23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=Y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600592a9-446f-4092-83a4-989f4940d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar histórico, pesos e modelo\n",
    "with open(\"history.json\", \"w\") as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "model.save_weights(\"pesos_modelo.weights.h5\")\n",
    "model.save(\"modelo.keras\")\n",
    "\n",
    "# Carregar modelo salvo\n",
    "loaded_model = load_model(\"modelo.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2706fb-a946-42e6-bec9-0eb683d53728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar otimizador rmsprop (apenas uma vez)\n",
    "rmsprop = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429688e9-7e76-4014-a4e7-ae5cb695f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar o modelo com a função de perda correta\n",
    "loaded_model.compile(\n",
    "    optimizer=rmsprop,\n",
    "    loss='binary_crossentropy',  # Utilize 'binary_crossentropy' para classificação binária\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f694fc-e404-4db1-a83d-47b0045c0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir a acurácia do modelo carregado\n",
    "accuracy = loaded_model.evaluate(X_test, Y_test, verbose=0)[1]  # Use os dados de teste aqui\n",
    "print(f\"Acurácia do modelo carregado: {accuracy * 100:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e573fb-445a-4b21-8d93-944ee862a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar histórico\n",
    "with open(\"history.json\", \"r\") as f:\n",
    "    history_data = json.load(f)\n",
    "\n",
    "loss = history_data['loss']\n",
    "val_loss = history_data['val_loss']\n",
    "acc = history_data['accuracy']\n",
    "val_acc = history_data['val_accuracy']\n",
    "\n",
    "epochs = list(range(len(loss)))\n",
    "figsize=(6,4)\n",
    "fig, axis1 = plt.subplots(figsize=figsize)\n",
    "\n",
    "plot1_lacc = axis1.plot(epochs, acc, 'navy', label='accuracy')\n",
    "plot1_val_lacc = axis1.plot(epochs, val_acc, 'deepskyblue', label=\"validation accuracy\")\n",
    "plot1_loss = axis1.plot(epochs, loss, 'red', label='loss')\n",
    "plot1_val_loss = axis1.plot(epochs, val_loss, 'lightsalmon', label=\"validation loss\")\n",
    "\n",
    "plots = plot1_loss + plot1_val_loss + plot1_lacc + plot1_val_lacc\n",
    "labs = [l.get_label() for l in plots]\n",
    "axis1.set_xlabel('Epoch')\n",
    "axis1.set_ylabel('Loss/Accuracy')\n",
    "plt.title(\"Loss/Accuracy History (Pristine Images)\")\n",
    "plt.tight_layout()\n",
    "axis1.legend(loc='center right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
